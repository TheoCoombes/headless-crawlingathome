{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CRAWLING@HOME - GPU VERSION ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V8yU0f_DLvg",
        "outputId": "5c48c167-e5e5-462b-a875-7bd98529351c"
      },
      "source": [
        "# HELP US TO BUILD A BILLION-SCALE IMAGE-CAPTION-DATASET BY FILTERING COMMONCRAWL (https://commoncrawl.org/) WITH CLIP  :)\n",
        "\n",
        "# JUST FILL IN YOUR NICKNAME AND \"RUN ALL\".\n",
        "# KEEP IT RUNNING AS LONG AS POSSIBLE & RESTART ONCE YOU GET DISCONNECTED\n",
        "\n",
        "YOUR_NICKNAME_FOR_THE_LEADERBOARD = \"rvencu\"\n",
        "CRAWLINGATHOME_SERVER_URL = \"http://crawlingathome.duckdns.org/\"\n",
        "\n",
        "# YOU CAN SEE YOUR CONTRIBUTIONS HERE\n",
        "# http://34.72.3.121/leaderboard\n",
        "\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.4\n",
        "\n",
        "!git clone \"https://github.com/TheoCoombes/crawlingathome\"\n",
        "!pip install -r crawlingathome/requirements.txt\n",
        "\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "\n",
        "#!pip3 install git+https://github.com/openai/CLIP.git\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install ftfy\n",
        "\n",
        "!git clone https://github.com/LuminosoInsight/python-ftfy\n",
        "\n",
        " \n",
        "!pip install cairosvg\n",
        " \n",
        "#!pip install googletrans==3.1.0a0\n",
        "!pip install spacy==2.2.4\n",
        "!pip install spacy-langdetect\n",
        " \n",
        "!pip install langid\n",
        " \n",
        "#!pip install -U easynmt\n",
        "!pip install tfr_image\n",
        " \n",
        "#!pip install vectorhub[clip]\n",
        " \n",
        "!pip install py7zr\n",
        "\n",
        "!pip install grequests\n",
        "\n",
        "!pip install atpbar\n",
        " \n",
        "import py7zr\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
        "\n",
        "# Change the current working directory\n",
        "os.chdir('./python-ftfy')\n",
        "\n",
        "\n",
        "!python ./setup.py install\n",
        "import ftfy\n",
        "ftfy.fix_text('âœ” No problems')\n",
        "\n",
        "os.chdir('../')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.5.0:\n",
            "  Successfully uninstalled tensorflow-2.5.0\n",
            "Collecting tensorflow==2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (2.5.0)\n",
            "Collecting h5py~=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.12.4)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.6.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.15.0)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 57.2MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (1.1.0)\n",
            "Collecting grpcio~=1.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4) (0.36.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.30.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (57.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4) (3.1.0)\n",
            "Installing collected packages: h5py, tensorflow-estimator, gast, grpcio, tensorflow\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: grpcio 1.34.1\n",
            "    Uninstalling grpcio-1.34.1:\n",
            "      Successfully uninstalled grpcio-1.34.1\n",
            "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0\n",
            "Cloning into 'crawlingathome'...\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (229/229), done.\u001b[K\n",
            "remote: Compressing objects: 100% (220/220), done.\u001b[K\n",
            "remote: Total 229 (delta 129), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (229/229), 55.95 KiB | 11.19 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r crawlingathome/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r crawlingathome/requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r crawlingathome/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r crawlingathome/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r crawlingathome/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r crawlingathome/requirements.txt (line 1)) (2020.12.5)\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████▋                        | 275.9MB 1.5MB/s eta 0:09:59"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtYBCT7lputf"
      },
      "source": [
        "# Experimental speedups (OPTIONAL) - could potentially increase CPU worker speed significantly.\n",
        "\n",
        "!yes | pip uninstall pillow\n",
        "!CC=\"cc -mavx2\" pip install -U --force-reinstall pillow-simd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYfmNGPqqGY3"
      },
      "source": [
        "import crawlingathome as cah\n",
        "\n",
        "client = cah.init(\n",
        "    url=CRAWLINGATHOME_SERVER_URL,\n",
        "    nickname= YOUR_NICKNAME_FOR_THE_LEADERBOARD\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ds2k6FUFYvUZ"
      },
      "source": [
        "import json, requests\n",
        "from pathlib import Path\n",
        "\n",
        "def refreshToken(client_id, client_secret, refresh_token):\n",
        "    params = {\n",
        "        \"grant_type\": \"refresh_token\",\n",
        "        \"client_id\": client_id,\n",
        "        \"client_secret\": client_secret,\n",
        "        \"refresh_token\": refresh_token\n",
        "    }\n",
        "\n",
        "    authorization_url = \"https://www.googleapis.com/oauth2/v4/token\"\n",
        "\n",
        "    r = requests.post(authorization_url, data=params)\n",
        "\n",
        "    if r.ok:\n",
        "        return r.json()['access_token']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        " \n",
        "def uploadGdrive(output_filename):\n",
        "    #output_filename = Path(output_filename).name\n",
        "\n",
        "    access_t = refreshToken(\"648172777761-onv1nc5f93nhlhf63flsq6onrmjphpfo.apps.googleusercontent.com\",\"HZ4Zw-_jVJ-3mwicz1NM5W5x\", \"1//04N2Kysz1LObLCgYIARAAGAQSNwF-L9IrntHNWi2_nEVu2QX5fmlW0Ea0qA-ToBJLSdatDATYxiKcNFI8eZQ_fYN53gjF7b8MGmA\")                                                                                                   \n",
        "\n",
        "    headers = {\"Authorization\": \"Bearer \" + access_t} #put ur access token after the word 'Bearer '\n",
        "\n",
        "    para = {\n",
        "        \"name\": output_filename.split(\"/\")[-1], # file name to be uploaded\n",
        "        \"parents\": [\"1CIgcIR7nX2xNBPB577jwEqbbwxAJR_nt\"] # make a folder on drive in which you want to upload files; then open that folder; the last thing in present url will be folder id\n",
        "    }\n",
        "    \n",
        "    files = {\n",
        "        'data': ('metadata', json.dumps(para), 'application/json; charset=UTF-8'),\n",
        "        'file': ('application/zip',open( output_filename , \"rb\")) # replace                        \n",
        "    }\n",
        "    r = requests.post(\n",
        "        \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\",\n",
        "        headers=headers,\n",
        "        files=files\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSyNVh_tRYV0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su_wv9987ted"
      },
      "source": [
        "while client.jobCount() > 0:\n",
        "\n",
        "    !mkdir ./save\n",
        "    !rm ./save/*\n",
        "    \n",
        "    \n",
        "    !mkdir ./save/images\n",
        "    !rm ./save/images/*\n",
        "\n",
        "    !mkdir ./finished\n",
        "    !rm ./finished/*\n",
        "\n",
        "\n",
        "    # Crawling@Home\n",
        "    client.newJob()\n",
        "    client.downloadShard() # Shard is located at './shard.wat'\n",
        "    \n",
        "    \n",
        "    output_folder= \"./save/\" \n",
        "    csv_output_folder = output_folder\n",
        "    img_output_folder = \"./save/images/\"  # these image files will be deleted after creating the arrays with CLIP preprocessing\n",
        "    \n",
        "    FIRST_SAMPLE_ID_IN_SHARD = client.start_id\n",
        "    LAST_SAMPLE_ID_IN_SHARD = client.end_id\n",
        "    N_SAMPLES_IN_SHARD = LAST_SAMPLE_ID_IN_SHARD -FIRST_SAMPLE_ID_IN_SHARD \n",
        "    shard_of_chunk = client.shard_piece  # should have values 0 - 1 ; says which 50% of a chunk will be processed\n",
        "    \n",
        "\n",
        "    import multiprocessing\n",
        "    n_processes = multiprocessing.cpu_count() * 2\n",
        "    \n",
        "    similarity_threshold = 0.3\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "    from atpbar import atpbar\n",
        "    from atpbar import register_reporter, find_reporter, flush\n",
        "    \n",
        "    import gc\n",
        "    #import torch.nn as nn\n",
        "    #cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    \n",
        "    from zipfile import ZipFile\n",
        "    import zipfile\n",
        "    import zlib\n",
        "    import pickle\n",
        "    \n",
        "    \n",
        "    content = []\n",
        "    counter = 0\n",
        "    with open('shard.wat', 'rb') as infile:\n",
        "        for line in infile:\n",
        "            counter += 1\n",
        "            content.append(line)\n",
        "    #hard_of_chunk= 0\n",
        "    if shard_of_chunk ==0: \n",
        "        content= content[int(len(content)*0.0):int(len(content)*0.5)]     \n",
        "    if shard_of_chunk ==1: \n",
        "        content= content[int(len(content)*0.5):]    \n",
        "\n",
        "\n",
        "    def worker(content,index_w, FIRST_SAMPLE_ID_IN_CHUNK, csv_output_folder,img_output_folder, n_processes, reporter):\n",
        "    \n",
        "        time_out=0.8\n",
        "        import grequests\n",
        "        import os\n",
        "        import time\n",
        "        import json\n",
        "        from collections import OrderedDict\n",
        "        import cairosvg\n",
        "        import pickle\n",
        "    \n",
        "        #print(\"Number of lines in the chunk: \" + str(len(content)))\n",
        "        from pathlib import Path\n",
        "    \n",
        "        import spacy\n",
        "        from spacy_langdetect import LanguageDetector\n",
        "        import langid\n",
        "        import time\n",
        "        from IPython.display import clear_output \n",
        "    \n",
        "        processed_contentlines= 0\n",
        "        urls=[]\n",
        "        alttexts=[]\n",
        "    \n",
        "        dedupe_urls=[]\n",
        "        dedupe_alttexts=[]\n",
        "    \n",
        "        processed_samples =[]\n",
        "        #translator = Translator()\n",
        "    \n",
        "        CURRENT_SAMPLE_ID=FIRST_SAMPLE_ID_IN_CHUNK\n",
        "        #alt_text_count = 0\n",
        "        #working_alt_text_count = 0\n",
        "    \n",
        "        #start_time = time.time()\n",
        "        for line in atpbar(content, name = \"Scraping worker #\"+str(index_w)):\n",
        "        #print(line)\n",
        "          line_str =line.decode(\"utf-8\")\n",
        "          alt_text_result=0\n",
        "          img_result=0\n",
        "    \n",
        "          img_result =line_str.find(\"IMG@\")\n",
        "          processed_contentlines += 1\n",
        "          if img_result>0:\n",
        "            alt_text_result = line_str.find(\"alt\\\":\")\n",
        "            \n",
        "            if alt_text_result>0:\n",
        "              #print(line_str)\n",
        "              data = json.loads(line_str)\n",
        "              \n",
        "              linklist= data['Envelope']['Payload-Metadata']['HTTP-Response-Metadata']['HTML-Metadata']['Links']\n",
        "              #print(data['Envelope']['Payload-Metadata']['HTTP-Response-Metadata']['HTML-Metadata'])\n",
        "    \n",
        "              for e in linklist:\n",
        "                if \"alt\" in e:\n",
        "                  \n",
        "                  if len(e[\"alt\"]) >4:\n",
        "                    #print(e[\"alt\"])\n",
        "    \n",
        "                    try:\n",
        "                      if e[\"url\"][0] ==\"h\" and not e[\"url\"] in dedupe_urls and not e[\"alt\"] in dedupe_alttexts:\n",
        "                        \n",
        "                        urls.append(e[\"url\"])\n",
        "                        alttexts.append(e[\"alt\"].encode(\"ascii\", \"ignore\").decode())\n",
        "                        dedupe_urls.append(e[\"url\"])\n",
        "                        dedupe_alttexts.append(e[\"alt\"].encode(\"ascii\", \"ignore\").decode())\n",
        "                        #print(\"***\")\n",
        "                        #print(\"len(urls) \"+str(len(urls)))\n",
        "                        #print(\"len(alttexts) \"+str(len(alttexts)))\n",
        "                        #print(\"len(e[alt]) \"+str(len(e[\"alt\"])))\n",
        "                \n",
        "                    except:\n",
        "                      continue\n",
        "    \n",
        "          if len(urls)>2000:\n",
        "            \n",
        "                \n",
        "            try:\n",
        "              # Once the last line of content is filtered, send the last requests\n",
        "              rs = (grequests.get(u, timeout=time_out) for u in urls)\n",
        "    \n",
        "              responses = grequests.map(rs)\n",
        "            except:\n",
        "              continue\n",
        "    \n",
        "            for i in range (len(responses)):\n",
        "              try:\n",
        "                \n",
        "                if responses[i].status_code == 200 and  responses[i].headers['Content-Type'][:3]==\"ima\" and len(responses[i].content)>5000:\n",
        "    \n",
        "                    #print(urls[i])\n",
        "                    filetype = Path(urls[i]).suffix    #os.path.splitext(e[\"url\"])[1]\n",
        "                    #print(filetype)\n",
        "    \n",
        "                    if len(filetype)<4:\n",
        "                      continue\n",
        "    \n",
        "                    if filetype[:4] ==\".jpg\" or filetype[:4] ==\".JPG\" or filetype[:4] ==\".png\" or filetype[:4] ==\".PNG\" or filetype[:4] ==\".svg\" or filetype[:4] ==\".SVG\" or filetype[:4] ==\".gif\" or filetype[:4] ==\".GIF\" or filetype[:4] ==\".bmp\" or filetype[:4] ==\".BMP\" or filetype[:4] ==\".tif\" or filetype[:4] ==\".TIF\":\n",
        "                      if len(filetype)>4:\n",
        "                        filetype = filetype[:4] \n",
        "    \n",
        "    \n",
        "                    if (filetype[:5] ==\".webp\" or filetype[:5] ==\".WEBP\" or filetype[:5] ==\".jpeg\" or filetype[:5] ==\".JPEG\")  and len(filetype)>5:\n",
        "                      filetype = filetype[:5] \n",
        "    \n",
        "                    if filetype[:4] != \".jpg\" and filetype[:4] != \".JPG\" and filetype[:4] != \".png\" and filetype[:4] !=\".PNG\" and filetype[:4] !=\".svg\" and filetype[:4] !=\".SVG\" and filetype[:4] !=\".gif\" and filetype[:4] != \".GIF\" and filetype[:4] !=\".bmp\" and filetype[:4] !=\".BMP\" and filetype[:4] !=\".tif\" and filetype[:4] !=\".TIF\"and filetype[:5] !=\".webp\" and filetype[:5] !=\".WEBP\" and filetype[:5] !=\".jpeg\" and filetype[:5] !=\".JPEG\":\n",
        "                      continue\n",
        "                        \n",
        "                    if filetype == \".svg\":\n",
        "                      #print(\"SVG found\")\n",
        "                      try:\n",
        "                        output_filename= img_output_folder + str(CURRENT_SAMPLE_ID)+filetype\n",
        "                        cairosvg.svg2png( url=urls[i], write_to=output_filename, output_height=600)\n",
        "                        #print(\"SVG converted\")\n",
        "                      except:\n",
        "                        continue\n",
        "                    else:\n",
        "                      try:\n",
        "                          img_data = responses[i].content\n",
        "                          with open(img_output_folder + str(CURRENT_SAMPLE_ID)+filetype, 'wb') as handler:\n",
        "                              handler.write(img_data)\n",
        "                          #print(\"Saved: \"+img_output_folder + str(CURRENT_SAMPLE_ID)+filetype)\n",
        "                      except:\n",
        "                        continue\n",
        "    \n",
        "                    processed_samples.append([CURRENT_SAMPLE_ID, urls[i], alttexts[i]])\n",
        "                    \n",
        "                    CURRENT_SAMPLE_ID +=1\n",
        "                    \n",
        "                  \n",
        "              except:\n",
        "                continue\n",
        "    \n",
        "            urls=[]\n",
        "            alttexts=[]\n",
        "    \n",
        "    \n",
        "            \n",
        "    \n",
        "            #print(\"Worker \"+str(index_w)+\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "            \n",
        "            #print(\"processed_samples-len: \"+str(len(processed_samples)))\n",
        "            #print(\"processed_samples[0]-len: \"+str(len(processed_samples[0])))\n",
        "    \n",
        "            #print(\"Currently processed content lines: \"+str(processed_contentlines))\n",
        "    \n",
        "            #clear_output()\n",
        "        #print(\"last filtering\")\n",
        "        #print(len(urls))\n",
        "        try:\n",
        "          # Once the last line of content is filtered, send the last requests\n",
        "          rs = (grequests.get(u, timeout=time_out) for u in urls)\n",
        "    \n",
        "          responses = grequests.map(rs)\n",
        "        except:\n",
        "          responses=[]\n",
        "        \n",
        "        #print(\"len(responses): \" + str(len(responses)))\n",
        "        #print(responses)\n",
        "        for i in range (len(responses)):\n",
        "          if responses[i]==None:\n",
        "            continue\n",
        "          #print(i)\n",
        "          #print(responses[i].status_code)\n",
        "          #print(responses[i].headers['Content-Type'][:3])\n",
        "          #print(len(responses[i].content))\n",
        "          try:\n",
        "            \n",
        "            if responses[i].status_code == 200 and  responses[i].headers['Content-Type'][:3]==\"ima\" and len(responses[i].content)>5000:\n",
        "                #print(urls[i])\n",
        "                filetype = Path(urls[i]).suffix \n",
        "                if len(filetype)<4:\n",
        "                  continue\n",
        "    \n",
        "                if filetype[:4] ==\".jpg\" or filetype[:4] ==\".JPG\" or filetype[:4] ==\".png\" or filetype[:4] ==\".PNG\" or filetype[:4] ==\".svg\" or filetype[:4] ==\".SVG\" or filetype[:4] ==\".gif\" or filetype[:4] ==\".GIF\" or filetype[:4] ==\".bmp\" or filetype[:4] ==\".BMP\" or filetype[:4] ==\".tif\" or filetype[:4] ==\".TIF\":\n",
        "                  if len(filetype)>4:\n",
        "                    filetype = filetype[:4] \n",
        "    \n",
        "                if (filetype[:5] ==\".webp\" or filetype[:5] ==\".WEBP\" or filetype[:5] ==\".jpeg\" or filetype[:5] ==\".JPEG\")  and len(filetype)>5:\n",
        "                  filetype = filetype[:5] \n",
        "    \n",
        "                if filetype[:4] != \".jpg\" and filetype[:4] != \".JPG\" and filetype[:4] != \".png\" and filetype[:4] !=\".PNG\" and filetype[:4] !=\".svg\" and filetype[:4] !=\".SVG\" and filetype[:4] !=\".gif\" and filetype[:4] != \".GIF\" and filetype[:4] !=\".bmp\" and filetype[:4] !=\".BMP\" and filetype[:4] !=\".tif\" and filetype[:4] !=\".TIF\"and filetype[:5] !=\".webp\" and filetype[:5] !=\".WEBP\" and filetype[:5] !=\".jpeg\" and filetype[:5] !=\".JPEG\":\n",
        "                  continue\n",
        "    \n",
        "    \n",
        "                if filetype == \".svg\":\n",
        "                  #print(\"SVG found\")\n",
        "                  try:\n",
        "                    output_filename= img_output_folder + str(CURRENT_SAMPLE_ID)+filetype\n",
        "                    cairosvg.svg2png( url=urls[i], write_to=output_filename, output_height=600)\n",
        "                    #print(\"Saved: \"+img_output_folder + str(CURRENT_SAMPLE_ID)+filetype)\n",
        "                  except:\n",
        "                    continue\n",
        "                else:\n",
        "                  try:\n",
        "    \n",
        "                    img_data = responses[i].content\n",
        "                    with open(img_output_folder + str(CURRENT_SAMPLE_ID)+filetype, 'wb') as handler:\n",
        "                        handler.write(img_data)\n",
        "                    #print(responses[i].headers)\n",
        "    \n",
        "                    \n",
        "                    #print(\"Saved: \"+img_output_folder + str(CURRENT_SAMPLE_ID)+filetype)\n",
        "    \n",
        "                  except:\n",
        "                    continue\n",
        "    \n",
        "                processed_samples.append([CURRENT_SAMPLE_ID, urls[i], alttexts[i] ])\n",
        "                CURRENT_SAMPLE_ID +=1\n",
        "                \n",
        "        \n",
        "                \n",
        "    \n",
        "          except:\n",
        "            continue\n",
        "    \n",
        "    \n",
        "        import pandas as pd\n",
        "        sample_ids= []\n",
        "        sample_urls=[]\n",
        "        sample_texts=[]\n",
        "\n",
        "        nlp = spacy.load('en')\n",
        "        nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
        "        for e in processed_samples:\n",
        "\n",
        "          text= e[2]\n",
        "          doc = nlp(text[:76])\n",
        "          # document level language detection. Think of it like average language of the document!\n",
        "          lang_detection = doc._.language\n",
        "          \n",
        "          lang_detection2 = langid.classify(text[:76])\n",
        "          if lang_detection[\"language\"] == \"en\" or lang_detection2[0]==\"en\":\n",
        "\n",
        "            sample_ids.append(e[0])\n",
        "            sample_urls.append(e[1])\n",
        "            sample_texts.append(e[2])\n",
        "\n",
        "        df = pd.DataFrame(list(zip(sample_ids,sample_urls,sample_texts)),   columns =['SAMPLE_ID', 'URL', 'TEXT'])\n",
        "\n",
        "        output_filename= csv_output_folder +\"FIRST_SAMPLE_ID_\"+str(FIRST_SAMPLE_ID_IN_CHUNK) + \"__LAST_SAMPLE_ID_\"+ str(CURRENT_SAMPLE_ID-1)+\"_\"+str(n_processes)+\".csv\"\n",
        "      \n",
        "        df.to_csv(output_filename ,sep = '|',header=True, mode='w', index=False)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "        \n",
        "    client.log(\"Downloading Images\")\n",
        "    #start_time = time.time()\n",
        "    jobs = []\n",
        "    reporter = find_reporter()\n",
        "    for i in range(n_processes):\n",
        "        content_chunk =content [i * int(len(content)/n_processes) : (i+1) * int(len(content)/n_processes)] #i*1000000\n",
        "        FIRST_SAMPLE_ID_IN_CHUNK = FIRST_SAMPLE_ID_IN_SHARD+ i *int(N_SAMPLES_IN_SHARD/n_processes)\n",
        "        p = multiprocessing.Process(target=worker, args=[content_chunk,i,FIRST_SAMPLE_ID_IN_CHUNK,csv_output_folder, img_output_folder, n_processes, reporter])\n",
        "        jobs.append(p)\n",
        "        p.start()\n",
        "\n",
        "    for p in jobs:\n",
        "        p.join()\n",
        "    \n",
        "    import gc\n",
        "    for p in jobs:\n",
        "        del p\n",
        "    gc.collect()\n",
        "        \n",
        "    \n",
        "\n",
        "    ###### \n",
        "\n",
        "    def imgfiles_to_embeddings(list_of_files, batch_size, model, preprocess):\n",
        "      if batch_size<2:\n",
        "        print(\"Minimal batch_size is 2 \")\n",
        "        return []\n",
        "\n",
        "      import numpy as np\n",
        "      from PIL import Image\n",
        "\n",
        "      import time\n",
        "\n",
        "      import os\n",
        "      #print(os.getcwd())\n",
        "      #os.chdir('./dm-haiku')\n",
        "      #import haiku as hk\n",
        "      #import torch\n",
        "      #import clip\n",
        "      #from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "      #model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "      #image = preprocess(Image.open(\"/content/V-300.png\")).unsqueeze(0).to(device)\n",
        "      #text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "      counter_samples =0\n",
        "\n",
        "      list_of_arrays_to_concat = []\n",
        "      list_of_tokenized_text_arrays =[]\n",
        "      list_of_arrays_to_concat = []\n",
        "      list_of_image_arrays =[]\n",
        "      list_of_tokenized_text_arrays =[]\n",
        "      img_embeddings= []\n",
        "\n",
        "\n",
        "\n",
        "      #devices =  jax.local_devices()\n",
        "\n",
        "      #print(f\"jax devices: {devices}\")\n",
        "\n",
        "      #jax_params = jax.device_put_replicated(jax_params, devices)\n",
        "      #image_fn = jax.pmap(image_fn)\n",
        "\n",
        "      for img_path in list_of_files:\n",
        "\n",
        "        try:\n",
        "          new_image_array = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "        except:\n",
        "          new_image_array = preprocess(Image.new(\"RGB\", (300, 300), (255, 255, 255))).unsqueeze(0).to(device)\n",
        "\n",
        "        #new_image_array = np.expand_dims(jax_preprocess(Image.open(img_path)), (0, 1))\n",
        "        #print(\"new_image_array = np.expand_dims(jax_preprocess(Image.open(img_path)), (0, 1))\")\n",
        "        #print(new_image_array.shape)  torch.cat((image,image,image,image,image,image,image,image,image,image), 0)\n",
        "        #except:\n",
        "        #  list_of_samples_to_drop.append(sample_id)\n",
        "        #  print(\"dropped sample: \"+str(sample_id))\n",
        "        #  continue\n",
        "\n",
        "\n",
        "        if counter_samples%batch_size ==0:\n",
        "          image_array =new_image_array\n",
        "          #tokenized_text_np_array = tokenized_text_np_array_new_sample\n",
        "          counter_samples +=1\n",
        "          continue\n",
        "        else:\n",
        "          image_array =  torch.cat((image_array,new_image_array), 0)\n",
        "          counter_samples +=1\n",
        "          #print(image_array.shape)\n",
        "\n",
        "\n",
        "        \n",
        "        if counter_samples%batch_size ==0:\n",
        "            with torch.no_grad():\n",
        "              image_features = model.encode_image(image_array)\n",
        "              \n",
        "\n",
        "              for i in range (image_features.shape[0]):\n",
        "                  img_embeddings.append(torch.reshape(image_features[i], (1, 512)))\n",
        "                  #img_embeddings.append(image_features[i])\n",
        "                  #print(torch.reshape(image_features[i], (1, 512)) .shape)\n",
        "      with torch.no_grad():\n",
        "          image_features = model.encode_image(image_array)\n",
        "          for i in range (image_features.shape[0]):\n",
        "            img_embeddings.append(torch.reshape(image_features[i], (1, 512)))\n",
        "            #img_embeddings.append(image_features[i])\n",
        "\n",
        "      #print(len(img_embeddings))\n",
        "      #print(img_embeddings[0].shape)\n",
        "      return img_embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    import sys\n",
        "    \n",
        "    \n",
        "    import glob\n",
        "    import pandas as pd\n",
        "    \n",
        "    #import psutil\n",
        "    \n",
        "    # Change the current working directory\n",
        "    #os.chdir('/content/dm-haiku')\n",
        "    #print(os.getcwd())\n",
        "    \n",
        "    from PIL import Image\n",
        "    import time\n",
        "    import numpy as np\n",
        "    \n",
        "    batch_size = 512 #2048\n",
        "    \n",
        "    \n",
        "    client.log(\"Saving CSV\")\n",
        "    \n",
        "    \n",
        "    counter_samples=0\n",
        "    \n",
        "    \n",
        "    #list_of_text_arrays_to_concat = []\n",
        "    \n",
        "    \n",
        "    #list_of_tokenized_text_arrays =[]\n",
        "    sample_ids = []\n",
        "    \n",
        "    #text_array_dict={}\n",
        "    \n",
        "    c=0\n",
        "    csv_files = glob.glob(output_folder + \"*.csv\")\n",
        "    #print(csv_files)\n",
        "    rows=0\n",
        "    for csv_file in csv_files:\n",
        "    \n",
        "        df = pd.read_csv(csv_file, sep = '|',lineterminator='\\n')\n",
        "            \n",
        "        rows += len(df)\n",
        "        if c ==0:\n",
        "            #print(c)\n",
        "            df.to_csv(output_folder + 'FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\"_\"+ str(shard_of_chunk)+'.csv',sep = '|',header=True, mode='a', index=False)\n",
        "        else:\n",
        "            #print(c)\n",
        "            df.to_csv(output_folder + 'FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\"_\"+ str(shard_of_chunk)+'.csv',sep = '|',header=False, mode='a', index=False)\n",
        "    \n",
        "        #print(rows)\n",
        "        c +=1\n",
        "        os.remove(csv_file)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #from sklearn.metrics.pairwise import cosine_similarity\n",
        "    #from vectorhub.bi_encoders.text_image.torch import Clip2Vec\n",
        "    \n",
        "    #model = Clip2Vec()\n",
        "\n",
        "    #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    import torch.nn as nn\n",
        "    cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    \n",
        "    csv_file=output_folder + 'FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\"_\"+ str(shard_of_chunk)+'.csv'\n",
        "    \n",
        "    \n",
        "    df = pd.read_csv(csv_file, sep = '|',lineterminator='\\n')\n",
        "    print (\"len(df) before filtering with clip\"+str(len(df)))\n",
        "    \n",
        "    img_files = glob.glob(img_output_folder + \"*.*\")\n",
        "    img_files_ids ={}\n",
        "    img_ids_by_filepath={} \n",
        "    for img_path in img_files:\n",
        "        path = Path(img_path)\n",
        "        path.name\n",
        "        img_files_ids[path.stem]= img_path\n",
        "        img_ids_by_filepath[img_path] = path.stem\n",
        "    \n",
        "    #print(img_files_ids)\n",
        "\n",
        "    import torch\n",
        "    import clip\n",
        "    from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    batch_size =512\n",
        "    img_emb_list= imgfiles_to_embeddings(img_files, batch_size, model, preprocess)\n",
        "\n",
        "    #print(\"len(img_files)\")\n",
        "    #print(len(img_files))\n",
        "\n",
        "\n",
        "    #print(\"len(img_emb_list)\")\n",
        "    #print(len(img_emb_list))\n",
        "\n",
        "    image_embedding_dict = {}\n",
        "\n",
        "    c= 0\n",
        "    for path in img_files:\n",
        "        img_sample_id = img_ids_by_filepath[path]\n",
        "        image_embedding_dict[img_sample_id] = img_emb_list[c]\n",
        "\n",
        "        c +=1\n",
        "\n",
        "    #print(\"len(image_embedding_dict)\")\n",
        "    #print(len(image_embedding_dict))\n",
        "\n",
        "\n",
        "    untokenized_texts=[]\n",
        "\n",
        "    tokenized_texts=[]\n",
        "    sample_ids_tokenized_texts=[]\n",
        "\n",
        "    text_embedding_list = []\n",
        "    for row_index, row in df.iterrows():\n",
        "        untokenized_texts.append (str( df.at[row_index,'TEXT']) [:75])\n",
        "        sample_ids_tokenized_texts.append (df.at[row_index,'SAMPLE_ID'])\n",
        "        if row_index% 128 ==0 and row_index >0:\n",
        "\n",
        "\n",
        "            tokenized_texts = clip.tokenize(untokenized_texts).to(device)\n",
        "            with torch.no_grad():\n",
        "              text_embeddings = model.encode_text(tokenized_texts)\n",
        "            for i in range(text_embeddings.shape[0]):\n",
        "              text_embedding_list.append(text_embeddings[i])\n",
        "\n",
        "            untokenized_texts=[]\n",
        "\n",
        "    if len(untokenized_texts)>0:      \n",
        "        tokenized_texts = clip.tokenize(untokenized_texts).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          text_embeddings = model.encode_text(tokenized_texts)\n",
        "        for i in range(text_embeddings.shape[0]):\n",
        "          text_embedding_list.append(text_embeddings[i])\n",
        "        untokenized_texts=[]\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    #texts_by_sample_id_dict[df.at[row_index,'SAMPLE_ID'] ] \n",
        "    #### NSFW detector categories text embeddings\n",
        "    \n",
        "    #0-18 /first 19 are not NSFW\n",
        "    nsfw_text_categories = [\"neutral\",\"selfie\", \"illustration, drawng\", \"toys, play, kids, children\", \"teddy bear, puppet\", \"animal, bird, mammal, insect\" \"fashion, clothes\", \"logo, commercial, ad, advertisement\", \"drawing, painting\",\"anime, cartoon\",\"comedy, fun\",\"romance, love story\",\"thriller, suspense, crime story\",\"action, action movie\", \"horror, monster movie\", \"documentary\", \"news, journalism\", \"entertainment\", \"talk show\", \"porn, sex, sperm, nipples, breats, tits, boops, penis, dick, cock, clitoris, vagina, fuck, lust, horny, sexual, lick, licking\",  \"porn, sex, sperm, nipples\", \"porn, sex, sperm, penis, dick, cock\", \"nipples, breats, tits, boops, sexy\", \"penis, dick, cock\", \"clitoris, vagina\", \"sex, fuck, lust, horny, sexual, lick, licking\", \"porn, sex, sexy\",\"sexy, hot\",\"sperm, skin\",\"lust, horny, sexual\",\"lick, licking, body\", \"anime, hentai, sexy\", \"cartoon, sexy, sex\", \"hentai\", \"anime, sexy, breasts\", \"hentai\"]\n",
        "\n",
        "    nsfw_text_tokenized = clip.tokenize(nsfw_text_categories).to(device)\n",
        "    nsfw_text_features =[]\n",
        "    with torch.no_grad():\n",
        "      nsfw_text_embed = model.encode_text(nsfw_text_tokenized)\n",
        "\n",
        "    for i in range(nsfw_text_embed.shape[0]):\n",
        "        nsfw_text_features.append(nsfw_text_embed[i])\n",
        "\n",
        "    #nsfw_text_features = np.array_split(nsfw_text_embed, len(nsfw_text_categories))\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    listofzeros = [\"-\"] * len(df)\n",
        "    \n",
        "    df[\"NSFW\"]=listofzeros\n",
        "    \n",
        "    \n",
        "    \n",
        "    #first 4 are underaged, 0-3\n",
        "    underaged_categories = [\"teenager, teen\", \"kid, child, teenager, teen, baby or toddler, underaged, little girl, little boy\", \"kid, child, little girl, little boy\", \"baby, toddler\",\"adult, woman, man, grownup, grown person,full-aged of legal age\",\"full-aged, of legal age, adult\",\"woman, man\",\"adult, woman, man, grownup, grown person,full-aged of legal age\"]\n",
        "    \n",
        "\n",
        "    underaged_text_tokenized = clip.tokenize(underaged_categories).to(device)\n",
        "    underaged_text_features =[]\n",
        "    with torch.no_grad():\n",
        "      underaged_text_embed = model.encode_text(underaged_text_tokenized)\n",
        "\n",
        "    for i in range(underaged_text_embed.shape[0]):\n",
        "        underaged_text_features.append(underaged_text_embed[i])\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    #0-20 /first 21 are not animals\n",
        "    animal_categories = [\"lifelss object, thing\", \"thing, object\", \"material\", \"furniture\",\"wall\", \"house\", \"tree\", \"wood\",\"ground\",\"industry\", \"table\", \"bed\", \"tool\", \"dress, clothes\", \"door\", \"chair\", \"rock, stone\", \"human\", \"man\", \"woman\", \"man, woman\", \"animal\",\"cat\",\"dog\", \"cow\", \"pig\", \"goat\", \"sheep\", \"elephant\", \"horse\", \"horse, elephant, pig, dog, cat, sheep, goat, animal\", \"life\", \"wildlife\"]\n",
        "    \n",
        "    animal_text_tokenized = clip.tokenize(animal_categories).to(device)\n",
        "    animal_text_features =[]\n",
        "    with torch.no_grad():\n",
        "      animal_text_embed = model.encode_text(animal_text_tokenized)\n",
        "\n",
        "    for i in range(animal_text_embed.shape[0]):\n",
        "        animal_text_features.append(animal_text_embed[i])\n",
        "\n",
        "\n",
        "    #print(len(animal_categories))  \n",
        "    #print(len(animal_text_features))  \n",
        "    ######### \n",
        "\n",
        "    \n",
        "    # given an iterable of pairs return the key corresponding to the greatest value\n",
        "    def argmax(pairs):\n",
        "        return max(pairs, key=lambda x: x[1])[0]\n",
        "    \n",
        "    # given an iterable of values return the index of the greatest value\n",
        "    def argmax_index(values):\n",
        "        return argmax(enumerate(values))\n",
        "    \n",
        "    \n",
        "    listofzeros = [0.0] * len(df)\n",
        "    \n",
        "    df[\"similarity\"]=listofzeros\n",
        "    \n",
        "    #image_embedding_dict= {}\n",
        "    #print (\"len(df)\"+str(len(df)))\n",
        "    \n",
        "    img_dict_counter= 0\n",
        "    #print (\"len(df) before 1st for row_index, row in df.iterrows():\"+str(len(df)))\n",
        "\n",
        "\n",
        "    client.log(\"Dropping NSFW Keywords\")\n",
        "    \n",
        "    \n",
        "    for row_index2, row2 in df.iterrows():\n",
        "        if str(df.at[row_index2,'TEXT']).lower().find(\"sex\") !=-1 or str(df.at[row_index2,'TEXT']).lower().find(\"nude\") !=-1  or  str(df.at[row_index2,'TEXT']).lower().find(\"sexy\") !=-1 or str(df.at[row_index2,'TEXT']).lower().find(\"fuck\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"orgasm\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"porn\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"lesbian\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"lust\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"pussy\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"bdsm\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"titts\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"horny\") !=-1   or str(df.at[row_index2,'TEXT']).lower().find(\"nacked\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"boops\") !=-1 or str(df.at[row_index2,'TEXT']).lower().find(\"erotic\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"lingerie\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"penis\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"dick\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"cock\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"dig\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"clit\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"nipple\") !=-1  or str(df.at[row_index2,'TEXT']).lower().find(\"gay\") !=-1  :\n",
        "            \n",
        "            if str(df.at[row_index2,'TEXT']).lower().find(\"teen\") !=-1 or str(df.at[row_index2,'TEXT']).lower().find(\"kid\") !=-1  or  str(df.at[row_index2,'TEXT']).lower().find(\"child\") !=-1 or str(df.at[row_index2,'TEXT']).lower().find(\"baby\") !=-1 :           \n",
        "            \n",
        "                #print(###########NSFW KEYWORD DROP##############)\n",
        "            \n",
        "                #print (df.at[row_index2,'TRANSLATION']))\n",
        "                df = df.drop(row_index2)\n",
        "                continue\n",
        "    \n",
        "    \n",
        "    similarity_counter= 0\n",
        "    for row_index, row in df.iterrows():\n",
        "        try:\n",
        "    \n",
        "        #if similarity_counter>500 and similarity_counter%10==0:\n",
        "            #!nvidia-smi\n",
        "    \n",
        "            if row_index % 100 ==0:\n",
        "                print(\"row_index: \"+ str(row_index))\n",
        "                client.log(f\"Removing NFSW: {row_index} / ?\")\n",
        "\n",
        "            sample_id = df.at[row_index,'SAMPLE_ID']\n",
        "            index_of_row_in_list= sample_ids_tokenized_texts.index(sample_id)\n",
        "            #print(\"index_of_row_in_list\")\n",
        "            #print(index_of_row_in_list)\n",
        "            if index_of_row_in_list==-1:\n",
        "                df = df.drop(row_index)\n",
        "                continue\n",
        "\n",
        "            current_text_embedding = text_embedding_list[index_of_row_in_list]\n",
        "            current_image_embedding = image_embedding_dict[str(sample_id)]\n",
        "            #print(\"current_image_embedding\")\n",
        "            #print(current_image_embedding.shape)\n",
        "            #print(\"current_text_embedding\")\n",
        "            #print(current_text_embedding.shape)\n",
        "            similarity= float (cosine_similarity(torch.reshape(current_text_embedding, (1, 512)) , current_image_embedding )) \n",
        "            print(df.at[row_index,'TEXT'])\n",
        "            print(df.at[row_index,'URL'])\n",
        "            print(\"similarity:\")\n",
        "\n",
        "            print(similarity)\n",
        "            if similarity > similarity_threshold:\n",
        "                df.at[row_index,'similarity'] = similarity\n",
        "                similarity_counter +=1\n",
        "\n",
        "\n",
        "\n",
        "                #0-18 /first 19 are not NSFW\n",
        "                nsfw_text_categories = [\"neutral\",\"selfie\", \"illustration, drawng\", \"toys, play, kids, children\", \"teddy bear, puppet\", \"animal, bird, mammal, insect\" \"fashion, clothes\", \"logo, commercial, ad, advertisement\", \"drawing, painting\",\"anime, cartoon\",\"comedy, fun\",\"romance, love story\",\"thriller, suspense, crime story\",\"action, action movie\", \"horror, monster movie\", \"documentary\", \"news, journalism\", \"entertainment\", \"talk show\", \"porn, sex, sperm, nipples, breats, tits, boops, penis, dick, cock, clitoris, vagina, fuck, lust, horny, sexual, lick, licking\",  \"porn, sex, sperm, nipples\", \"porn, sex, sperm, penis, dick, cock\", \"nipples, breats, tits, boops, sexy\", \"penis, dick, cock\", \"clitoris, vagina\", \"sex, fuck, lust, horny, sexual, lick, licking\", \"porn, sex, sexy\",\"sexy, hot\",\"sperm, skin\",\"lust, horny, sexual\",\"lick, licking, body\", \"anime, hentai, sexy\", \"cartoon, sexy, sex\", \"hentai\", \"anime, sexy, breasts\", \"hentai\"]\n",
        "                #nsfw_text_features = model.encode_text(nsfw_text_categories)\n",
        "                similarities=[]\n",
        "        \n",
        "                for i in range(len(nsfw_text_features)):\n",
        "                    similarity= float (cosine_similarity(torch.reshape(nsfw_text_features[i], (1, 512)) , current_image_embedding )) \n",
        "                    similarities.append( similarity )\n",
        "        \n",
        "                print(similarities)\n",
        "        \n",
        "                argmax1= argmax_index(similarities)\n",
        "                most_likely= nsfw_text_categories[argmax1]\n",
        "                print (\"most_likely\")\n",
        "                print (most_likely)\n",
        "        \n",
        "        \n",
        "                nsfw_text_categories.pop(argmax_index(similarities))\n",
        "                similarities.pop(argmax_index(similarities))\n",
        "                argmax2= argmax_index(similarities)\n",
        "                second_likely = nsfw_text_categories[argmax_index(similarities)]\n",
        "        \n",
        "                if argmax1 <19 and argmax2<19:\n",
        "                    df.at[row_index,'NSFW'] = \"UNLIKELY\"\n",
        "                elif argmax1 <19 and argmax2>=19:\n",
        "                    df.at[row_index,'NSFW'] = \"UNSURE\"\n",
        "                elif argmax2 <19 and argmax1>=19:\n",
        "                    df.at[row_index,'NSFW'] = \"UNSURE\"\n",
        "                elif argmax1 >=19 and argmax2>=19:\n",
        "                    df.at[row_index,'NSFW'] = \"NSFW\"\n",
        "        \n",
        "\n",
        "        \n",
        "                ####underaged check \n",
        "                if df.at[row_index,'NSFW'] != \"UNLIKELY\":\n",
        "        \n",
        "                    #keyword check\n",
        "                    if str(df.at[row_index,'TEXT']).lower().find(\"teen\") !=-1 or str(df.at[row_index,'TEXT']).lower().find(\"kid\") !=-1  or  str(df.at[row_index,'TEXT']).lower().find(\"child\") !=-1 or str(df.at[row_index,'TEXT']).lower().find(\"baby\") !=-1 :\n",
        "                        df = df.drop(row_index)\n",
        "                        print(###########NSFW KEYWORD DROP##############)\n",
        "                        print (df.at[row_index,'TEXT']))\n",
        "                        continue\n",
        "                    \n",
        "                    #first 4 are underaged, 0-3\n",
        "                    underaged_categories = [\"teenager, teen\", \"kid, child, teenager, teen, baby or toddler, underaged, little girl, little boy\", \"kid, child, little girl, little boy\", \"baby, toddler\",\"adult, woman, man, grownup, grown person,full-aged of legal age\",\"full-aged, of legal age, adult\",\"woman, man\",\"adult, woman, man, grownup, grown person,full-aged of legal age\", \"drawing, logo, clip art\", \"illustration, cartoon\", \"captcha, screen\", \"food, eating, meal, drink\", \"car\"]\n",
        "        \n",
        "                    similarities=[]\n",
        "        \n",
        "                    for i in range(len(underaged_text_features)):\n",
        "                        #similarities.append( cosine_similarity([underaged_text_features[i][0]], [current_image_embedding[0][0]]) )\n",
        "            \n",
        "                        similarity= float (cosine_similarity(torch.reshape(underaged_text_features[i], (1, 512)) , current_image_embedding )) \n",
        "                        similarities.append( similarity )        \n",
        "        \n",
        "                    argmax1= argmax_index(similarities)\n",
        "                    print(\"argmax1\")\n",
        "                    print(argmax1)\n",
        "                    most_likely= underaged_categories[argmax1]\n",
        "                    \n",
        "                    print (\"most_likely\")\n",
        "\n",
        "                    print (most_likely)\n",
        "        \n",
        "                    underaged_categories.pop(argmax_index(similarities))\n",
        "                    similarities.pop(argmax_index(similarities))\n",
        "                    argmax2= argmax_index(similarities)\n",
        "                    print(\"argmax2\")\n",
        "                    print(argmax2)\n",
        "                    second_likely = underaged_categories[argmax_index(similarities)]\n",
        "                    print(second_likely)\n",
        "                    if argmax1 <4 or argmax2 <4:\n",
        "                        #print( df.at[row_index,'URL'] )\n",
        "                        del image_embedding_dict[str(sample_id)]\n",
        "                        df = df.drop(row_index)\n",
        "\n",
        "                        print(\"dropped cause NSFW and eventually underaged\")\n",
        "                        \n",
        "                        continue\n",
        "        \n",
        "        \n",
        "                ####animal check \n",
        "                if df.at[row_index,'NSFW'] != \"UNLIKELY\":\n",
        "                    \n",
        "                    #0-20 /first 21 are not animals\n",
        "                    animal_categories = [\"lifelss object, thing\", \"thing, object\", \"material\", \"furniture\",\"wall\", \"house\", \"tree\", \"wood\",\"ground\",\"industry\", \"table\", \"bed\", \"tool\", \"dress, clothes\", \"door\", \"chair\", \"rock, stone\", \"human\", \"man\", \"woman\", \"man, woman\", \"animal\",\"cat\",\"dog\", \"cow\", \"pig\", \"goat\", \"sheep\", \"elephant\", \"horse\", \"horse, elephant, pig, dog, cat, sheep, goat, animal\", \"life\", \"wildlife\"]\n",
        "        \n",
        "                    similarities=[]\n",
        "        \n",
        "\n",
        "                    for i in range(len(animal_text_features)):\n",
        "                        #similarities.append( cosine_similarity([animal_text_features[i][0]], [current_image_embedding[0][0]]) )\n",
        "                        similarity= float (cosine_similarity(torch.reshape(animal_text_features[i], (1, 512)) , current_image_embedding )) \n",
        "                        similarities.append( similarity )       \n",
        "                    print (\"most_likely\")\n",
        "\n",
        "                    print (most_likely)\n",
        "        \n",
        "                    argmax1= argmax_index(similarities)\n",
        "                    most_likely= animal_categories[argmax1]\n",
        "        \n",
        "        \n",
        "                    #print(second_likely)\n",
        "                    if argmax1 >20:\n",
        "\n",
        "                        del image_embedding_dict[str(sample_id)]\n",
        "\n",
        "                        df = df.drop(row_index)\n",
        "                        print(\"dropped cause NSFW and eventually animal\")\n",
        "                        \n",
        "                        continue\n",
        "\n",
        "            else:\n",
        "                del image_embedding_dict[str(sample_id)]\n",
        "                df = df.drop(row_index)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(\"dropped sample: \"+str(df.at[row_index,'SAMPLE_ID']))\n",
        "            print(e)\n",
        "            print( \"embedding error\")\n",
        "            \n",
        "            try:\n",
        "                df = df.drop(row_index)\n",
        "            except:\n",
        "                print(\"WEIRD ERROR\")\n",
        "            continue\n",
        "    \n",
        "            \n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    client.log(\"Dumping Embeddings\")\n",
        "    for key in image_embedding_dict:\n",
        "      image_embedding_dict[key] = image_embedding_dict[key].cpu().detach().numpy()\n",
        "    \n",
        "    # save the last img_array_dict\n",
        "    with open(output_folder + \"image_embedding_dict\"+ '-FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\"_\"+ str(shard_of_chunk)+\".pkl\",\"wb\") as f:\n",
        "        pickle.dump(image_embedding_dict, f)\n",
        "    \n",
        "    '''\n",
        "    with ZipFile(output_folder + \"img_array_dict_\"+'FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD) +'.zip','w',zipfile.ZIP_DEFLATED) as zip:\n",
        "            zip.write(output_folder + \"image_embedding_dict\"+ '_FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\".pkl\")\n",
        "    os.remove(output_folder + \"image_embedding_dict\"+ '_FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\".pkl\")\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### converting core data to TFrecords\n",
        "    \n",
        "    \n",
        "    #!pip install tfr_image\n",
        "    \n",
        "    \n",
        "    from tfr_image import TFRimage\n",
        "    \n",
        "    from tfr_image.utils import (\n",
        "        get_filenames_and_classes,\n",
        "        write_label_file,\n",
        "        bytes_feature,\n",
        "        int64_feature,\n",
        "    )\n",
        "    \n",
        "    \n",
        "    def _get_dataset_filename(\n",
        "            dataset_dir, split_name, shard_id, tfrecord_filename, num_chards\n",
        "        ):\n",
        "            output_filename = \"%s_%s_%05d-of-%05d.tfrecord\" % (\n",
        "                tfrecord_filename,\n",
        "                split_name,\n",
        "                shard_id,\n",
        "                num_chards,\n",
        "            )\n",
        "            return os.path.join(dataset_dir, output_filename)\n",
        "    \n",
        "    \n",
        "    \n",
        "    def _image_to_tfexample( sampleID, image_data, image_format, height, width, caption):\n",
        "    \n",
        "    \n",
        "            return tf.train.Example(\n",
        "                features=tf.train.Features(\n",
        "                    feature={\n",
        "                        \"sampleID\": bytes_feature(sampleID),\n",
        "                        \"image\": bytes_feature(image_data),\n",
        "                        \"format\": bytes_feature(image_format),\n",
        "                        \"label\": bytes_feature(caption),#int64_feature(class_id),\n",
        "                        \"height\": int64_feature(height),\n",
        "                        \"width\": int64_feature(width),\n",
        "                    }\n",
        "    \n",
        "    \n",
        "                )\n",
        "            )\n",
        "    \n",
        "    \n",
        "    def _convert_dataset(\n",
        "            \n",
        "            split_name,\n",
        "            filenames,\n",
        "            sampleIDs,\n",
        "            captions,\n",
        "            dataset_dir,\n",
        "            tfrecord_filename,\n",
        "            num_chards,\n",
        "            df\n",
        "        ):\n",
        "            \"\"\"Converts the given filenames to a TFRecord dataset.\n",
        "            Args:\n",
        "            split_name: The name of the dataset, either 'train' or 'validation'.\n",
        "            filenames: A list of absolute paths to png or jpg images.\n",
        "            class_names_to_ids: A dictionary from class names (strings) to ids\n",
        "            (integers).\n",
        "            dataset_dir: The directory where the converted datasets are stored.\n",
        "            \"\"\"\n",
        "            #assert split_name in [\"train\", \"validation\"]\n",
        "    \n",
        "            num_per_shard = int(math.ceil(len(filenames) / float(num_chards)))\n",
        "    \n",
        "            for shard_id in range(num_chards):\n",
        "                #if shard_id % 100 == 0:\n",
        "                #    client.log(f\"Converting to TFRs: {shard_id} / {len(num_chards)}\")\n",
        "                \n",
        "                output_filename = _get_dataset_filename(\n",
        "                    dataset_dir,\n",
        "                    split_name,\n",
        "                    shard_id,\n",
        "                    tfrecord_filename=tfrecord_filename,\n",
        "                    num_chards=num_chards,\n",
        "                )\n",
        "                heights=[]\n",
        "                widths=[]\n",
        "                with tf.io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
        "                    start_ndx = shard_id * num_per_shard\n",
        "                    end_ndx = min((shard_id + 1) * num_per_shard, len(filenames))\n",
        "                    for i in range(len(filenames)):\n",
        "                        sys.stdout.write(\n",
        "                            \"\\r>> Converting image %d/%d shard %d\"\n",
        "                            % (i + 1, len(filenames), shard_id)\n",
        "                        )\n",
        "                        sys.stdout.flush()\n",
        "    \n",
        "                        # Read the filename:\n",
        "                        try:\n",
        "                            image_data = tf.io.gfile.GFile(filenames[i], \"rb\").read()\n",
        "                            with tf.io.gfile.GFile(filenames[i], \"rb\") as f:\n",
        "                                image = Image.open(f)\n",
        "                        except:\n",
        "                            df = df.drop(i)\n",
        "                            continue\n",
        "                        height, width = image.size\n",
        "                        heights.append(height)\n",
        "                        widths.append(width)\n",
        "                        #class_name = os.path.basename(os.path.dirname(filenames[i]))\n",
        "                        #class_id = class_names_to_ids[class_name]\n",
        "                        \n",
        "                        try:\n",
        "                            caption = captions[i].encode('utf_8')\n",
        "                            sampleID = sampleIDs[i].encode('utf_8')\n",
        "                        except:\n",
        "                            caption = captions[i]\n",
        "                            sampleID = sampleIDs[i]\n",
        "    \n",
        "                        example = _image_to_tfexample( sampleID,\n",
        "                            image_data, b\"jpg\", height, width, caption # class_id\n",
        "                        )\n",
        "                        tfrecord_writer.write(example.SerializeToString())\n",
        "            df.reset_index(drop=True, inplace=True)\n",
        "            sys.stdout.write(\"\\n\")\n",
        "            sys.stdout.flush()\n",
        "            return widths,heights, df\n",
        "    \n",
        "    \n",
        "    import random\n",
        "    import math\n",
        "    #import os\n",
        "    import sys\n",
        "    from PIL import Image\n",
        "    import tensorflow as tf\n",
        "    \n",
        "    \n",
        "    sampleIDs= []\n",
        "    image_filenames=[]\n",
        "    translations=[]\n",
        "    for row_index, row in df.iterrows():\n",
        "        sampleIDs.append(str(df.at[row_index,'SAMPLE_ID']))\n",
        "        translations.append(str(df.at[row_index,'TEXT']))\n",
        "        image_filenames.append(img_files_ids[str(df.at[row_index,'SAMPLE_ID'])])\n",
        "    \n",
        "    listofzeros = [\"-\"] * len(df)\n",
        "    \n",
        "    df[\"WIDTH\"]=listofzeros\n",
        "    df[\"HEIGHT\"]=listofzeros\n",
        "    \n",
        "    \n",
        "    widths,heights, df = _convert_dataset(split_name = \"\", sampleIDs = sampleIDs, filenames=image_filenames, captions=translations, dataset_dir=\"./save/\", tfrecord_filename=\"crawling_at_home_\"+ 'FIRST_SAMPLE_ID_IN_SHARD_'+str(FIRST_SAMPLE_ID_IN_SHARD)+\"_LAST_SAMPLE_ID_IN_SHARD_\"+str(LAST_SAMPLE_ID_IN_SHARD)+\"_\"+str(shard_of_chunk), num_chards=1, df=df)\n",
        "    #print(\"len(heights)\" + str(len(heights)))\n",
        "    #print(\"len(df)\" + str(len(df)))\n",
        "    \n",
        "    for row_index, row in df.iterrows():\n",
        "        df.at[row_index,'WIDTH'] = widths[row_index]\n",
        "        df.at[row_index,'HEIGHT'] =heights[row_index]\n",
        "    \n",
        "    client.log(\"Saving TFRs\")\n",
        "    \n",
        "    df.to_csv(csv_file ,sep = '|',header=True, mode='w', index=False)\n",
        "    #len (df)\n",
        "    \n",
        "    for key in img_files_ids:\n",
        "        os.remove(img_files_ids[key])\n",
        "    \n",
        "    '''\n",
        "    #remove the remaining img files of the samples that had beed dropped  \n",
        "    import shutil\n",
        "    shutil.rmtree(img_output_folder)\n",
        "    ''' \n",
        "\n",
        "    # Now we need to upload for Crawling@Home\n",
        "\n",
        "    from pathlib import Path\n",
        "\n",
        "    saves = Path(\"./save\")\n",
        "\n",
        "    client.log(\"Uploading CSV\")\n",
        "    uploadGdrive(f\"./save/FIRST_SAMPLE_ID_IN_SHARD_{FIRST_SAMPLE_ID_IN_SHARD}_LAST_SAMPLE_ID_IN_SHARD_{LAST_SAMPLE_ID_IN_SHARD}_\"+str(shard_of_chunk)+\".csv\")\n",
        "\n",
        "    tfrecords = [*saves.glob(\"*.tfrecord\")]\n",
        "    for i, f in enumerate(tfrecords):\n",
        "        client.log(f\"Uploading TFRs: {i + 1} / {len(tfrecords)}\")\n",
        "        uploadGdrive(str(f))\n",
        "    \n",
        "    client.log(\"Uploading Image Embeddings\")\n",
        "    uploadGdrive(f\"./save/image_embedding_dict-FIRST_SAMPLE_ID_IN_SHARD_{FIRST_SAMPLE_ID_IN_SHARD}_LAST_SAMPLE_ID_IN_SHARD_{LAST_SAMPLE_ID_IN_SHARD}_\"+str(shard_of_chunk)+\".pkl\")\n",
        "\n",
        "    client._markjobasdone(len(df))\n",
        "\n",
        "    print(\"Complete time --- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "\n",
        "#client.bye()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}